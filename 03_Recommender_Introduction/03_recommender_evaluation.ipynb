{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics, Methods and concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating recommender systems is not an easy task. The success of recommender system depends pretty much on the people testing it. It is relatively easy to optimize the underlying machine learning models but in the end the user is the one who makes a decision and it is hard to say if a person considers a recommendation good enough. This is the reason why a lot of people consider recommender systems as much as art as science. In this notebook you will get an overview of metrics and methods of how to evaluate your recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Offline metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in every other machine learning setting, the data is splitted into a training and a testing set. Common metrics and methods to evaluate recommender systems are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE\n",
    "\n",
    "The mean absolute error sums up the differences between each observation and the corresponding predicted value and averages it over the number of observations. \n",
    "\n",
    "$$\n",
    "  MAE = \\frac{\\sum_{i=1}^n |y_{i}-\\hat{y_{i}}|}{n}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE \n",
    "\n",
    "The RMSE is the squaroot of the sum of the squared difference between observation and prediction, averaged over the number of observations. Due to the squared error it penalizes outlier more than MAE.\n",
    "\n",
    "$$\n",
    "   RSME = \\sqrt\\frac{\\sum_{i=1}^n (y_{i}-\\hat{y_{i}})^2}{n}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hit rate\n",
    "\n",
    "To measure the hit rate the user is provided with the top-n recommendations. If the user actually rates one of the recommendations it is considered a hit. The hit rate then is the number of hits in your test-set divided by the number of users. To evaluate your recommender system via hit rate a method called leave-one-out cross validation is used. First the top-n recommendations are calculated for all users. To evaluate the algorithm one item is removed. Then it is checked for each user wether it would have been a hit. A limitation of this method is to get enough evaluation data for a specific item. Hence this method is more applicable to large data sets.\n",
    "\n",
    "$$\n",
    "  hit rate  = \\frac{n_{hits}}{n_{users}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average reciprocal hit rate (ARHR)\n",
    "\n",
    "The ARHR takes the rank of an item into account. This measure is useful if people have to scroll to find items in the top-n list.\n",
    "\n",
    "$$\n",
    "  ArHR = \\frac{\\sum_{i=1}1 - rank_i}{n_{users}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cumulative hit rate (cHR)\n",
    "\n",
    "For this aproach of evaluating a recommender system, the hitrate metric will not consider ratings below a certain threshold items in the top-n list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rating hit rate (rHR)\n",
    "\n",
    "This variant of the hit rate is based on predicted ratings for a user in the top-n recommendation list. As ratings are provided by machine learning algorithms, this metric also depends on the RMSE of the predicted ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coverage \n",
    "\n",
    "The coverage of a recommender system is defined as the % of possible recommendations your system is able to provide. So, it is the amount of rated movies in relation to the total amount of movies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversity\n",
    "\n",
    "The diversity is defined as 1 minus the average similarity between recommendation pairs. If diversity in the provided recommendations is to low, users may get bored or buying behavior lowers. Furthermore users might enjoy finding niche items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Novelty \n",
    "\n",
    "To build a successful recommender system, noverlty of items is an important concept. Users want to be provided with recent items but as those might not be rated yet, they'll possibly be provided with meaningless recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Churn\n",
    "\n",
    "Churn is a measure how user behavior changes over time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Online metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A/B Tests\n",
    "\n",
    "This is the method that matters most when developing recommender systems as it measures real user behaviour. It means, two different versions of the recommender system are tested against each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preceived feedback\n",
    "\n",
    "Get direct feedback from the user"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7c571947cfb866b878ed74aee86da3d4d50f96f1dd9dbfea7e297ce4396c8e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
